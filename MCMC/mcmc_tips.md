# ここでは、MCMCの知識・小技をまとめていく　　


### ＊Why MCMC?  
- なぜMCMCを使うのか？  
-- パラメータ次元が多すぎて、カイ2乗推定では、サーベイするパラメータレンジが膨大すぎて、計算が爆発してしまう。
かといって、勾配降下法では、高確率で、極所解に陥ってしまう(さらにエラーももとまらない)。**MCMCだと（完全ではないものの）広い空間を限られた時間で探索できる**。  

### ＊Parallel Tempering
- parallel temperingのHyperparameter(逆温度)の調整は、基本的に、Araki et al. 2013に従えばいい。ただし、レプリカの数は、パラメータの数のルート(e.g.,parameter number=50, replica number=7とかでいいのか、、)が「数学的には」最適らしい。　また、初期値は、exp(0), exp(1), exp(2)...とするのがいいらしい。　　

- 提案分布の分散の初期値は本来予想される値よりも大きめの値からスタートするのが良い、と聞いたことがある。学習して適切な値まで小さくなって行くので、最初から小さい値にすると学習の効果がない。逆温度の初期値に関しては、最小の値の設定がまずは重要。パラメータ空間を十分自由に動けるような値で最小の逆温度の値を設定すべきで、具体的な値は試行錯誤するしかないと思います。逆温度は一般的には「対数スケールで等間隔」に設定するのが良いそうだが、特に相転移が起きそうな温度付近は密に刻むべき。  

- temperingの逆温度は、adaptiveで動かすことが多いが、an=1/(burn_in*iteration_number + n)とかにするとよい。
ここで、nは要らないように見えるが、数学的には、an(n→∞)=0, Σan(n→∞)=∞である数列の方が好ましい。
ただし、adaptiveに学習する際の、数列係数の設定は、**非常に悩ましい**。色々試行錯誤して、今は最初からかなり小さな係数にして、正解に近いパラメータから始めて、時間をかけて学習することで良く機能する。

- Emerging-Dacay Rateも、Areaと同様に、共分散にした方がいいか？→どちらでもいい。  

- prior distributionを設定する上での注意点：  
-- Emerging-Decay rate, Areaに関しては、log分布??(名前忘れた)、p(a)=1/a/(log(a_max/a_min))にすると良い。  
-- この時、提案分布も、この形にしなければならない。つまり、Q(y,x) = 1/y*exp(-( ln(y)-ln(x) )/2σ^2)  
-- その場合、メトロポリスの、次を選ぶ確率のところに、Q = 1/y*exp(-( ln(y)-ln(x) )/2σ^2)を分母分子にかけないといけないので注意。  


### ＊提案分布について、
- 相関の強いパラメータがある場合は共分散を入れるのは必須だと思います。まずはその判断。


### ＊モデルの比較

- モデルの比較には、「**ベイズファクター**」を比較するのが良い。例えば、ベイスファクターは、B(n,n+1) = p(D|M2)/p(D|M1)として書かれる。
ここで、分母分子は、∮(Likelihood)×(Prior)dΘ(i.e.全空間積分)として計算できる。これが、「**大きいもの**」を選べば良い。　　
https://ja.wikipedia.org/wiki/%E3%83%99%E3%82%A4%E3%82%BA%E5%9B%A0%E5%AD%90　　

- ベイズ因子（ベイズいんし、英: Bayes factor）は、ベイズ統計学において、伝統的統計学の仮説検定に代わる方法として用いられる数値である。
データベクトルx に基づいて2つの数学的モデル M1 と M2 のどちらかを選択する問題を考える。
この方法は尤度比検定あるいは最尤法に似ているが、尤度（モデルあるいは母数を定数とし、それを条件とする確率変数x の条件付確率のこと）を最大化するのでなく、母数を確率変数とし、それに対して平均値をとってから最大化するところが違う。

- 基準は以下より、、  
https://www.stat.washington.edu/raftery/Research/PDF/kass1995.pdf　　  
簡単な例(広島カープ)を使って解説　　  
https://www.slideshare.net/kazutantan/bayes-factor　　




